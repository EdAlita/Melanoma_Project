{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, average_precision_score, f1_score, cohen_kappa_score, recall_score, log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Lars, ElasticNet, RidgeClassifier, BayesianRidge\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "\n",
    "time = strftime(\"%Y-%m-%d_%H-%M-%S\", gmtime())\n",
    "\n",
    "classifiers = [\n",
    "                RandomForestClassifier(criterion='entropy', max_depth=20, n_estimators=10, max_features=1),\n",
    "                KNeighborsClassifier(1),\n",
    "            ]\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "\n",
    "# Define a custom scoring function\n",
    "def custom_score(y_true, y_pred, fn=accuracy_score):\n",
    "    # Apply the threshold to obtain binary predictions\n",
    "    y_pred_binary = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "    # Calculate and return the custom metric\n",
    "    # Replace this with your own custom metric calculation\n",
    "    return fn(y_true, y_pred_binary)\n",
    "\n",
    "\n",
    "\n",
    "def eval_classifiers(X, y, labels, **kwargs):\n",
    "\n",
    "    cv_scorers = {\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "    # 'cross_entropy_loss': make_scorer(log_loss, labels=labels),\n",
    "    # 'average_precision_score' : make_scorer(average_precision_score, average='weighted', pos_label =0),\n",
    "    'cohen_kappa_score' : make_scorer(cohen_kappa_score, labels=labels),\n",
    "    'f1_score' : make_scorer(f1_score, average='weighted', labels=labels),\n",
    "    'recall_score' : make_scorer(recall_score, average='weighted', labels=labels),\n",
    "    # 'roc_auc_score': make_scorer(roc_auc_score, average='weighted', labels=labels, multi_class = 'ovr'),\n",
    "    # 'specificity_score' : make_scorer(recall_score, pos_label=0, average='binary', labels=labels),\n",
    "    }\n",
    "    # Define the list of scoring metrics\n",
    "    mean_res = pd.DataFrame()\n",
    "    std_res = pd.DataFrame()\n",
    "    \n",
    "    for i, clf in tqdm(enumerate(classifiers), desc=\"Classifiers are running....\"):\n",
    "        # ax = plt.subplot(len(classifiers) + 1, i)\n",
    "        clf_key = str(clf)\n",
    "        \n",
    "        \n",
    "        clf = Pipeline(steps=[('scaler',StandardScaler()),\n",
    "                            ('pca', PCA(n_components=500)), \n",
    "                            ('estimator',clf)])\n",
    "        \n",
    "        # Apply cross-validated model here.\n",
    "        cv = StratifiedKFold(n_splits=100, shuffle=True)  # Specify the number of desired folds\n",
    "        cv_scores = cross_validate(clf, X, y, cv=cv, scoring=cv_scorers, return_train_score=False, return_estimator=True, n_jobs=-1,verbose=2)  # Specify the list of scoring metrics\n",
    "        # print(cv_scores)\n",
    "        # print(np.array(cv_scores.values()))\n",
    "        estimators = cv_scores['estimator']\n",
    "\n",
    "        # Delete estimators\n",
    "        del cv_scores['estimator']\n",
    "        # Use sklearn metrics AUC.\n",
    "        for j, key in enumerate(cv_scores.keys()):\n",
    "            mean_res.loc[clf_key, key] = np.mean(cv_scores[key])\n",
    "            std_res.loc[clf_key, key] = np.std(cv_scores[key])\n",
    "\n",
    "    \n",
    "    return estimators, mean_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifiers are running....: 0it [00:00, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.8min finished\n",
      "Classifiers are running....: 1it [02:48, 168.55s/it][Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.5min finished\n",
      "Classifiers are running....: 2it [04:16, 121.34s/it][Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.5min finished\n",
      "Classifiers are running....: 3it [06:50, 135.87s/it][Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../features/all/features_train_HSV_GLCM_shape_MC.csv')\n",
    "\n",
    "if len(data['label'].unique()) == 2:\n",
    "    category_mapping = {'nevus': 1, 'others': 0} # Should we switch?\n",
    "    labels = [0, 1]\n",
    "\n",
    "else:\n",
    "    data_exc = data[data['label'].isin(['mel', 'bcc', 'scc'])]\n",
    "    category_mapping = {\n",
    "                        'mel': 0, \n",
    "                        'bcc': 1, \n",
    "                        'scc': 2, \n",
    "                        }\n",
    "    labels = [0, 1, 2]\n",
    "    \n",
    "\n",
    "y =  data_exc['label'].astype('category').map(category_mapping)\n",
    "\n",
    "X = data_exc.iloc[:, 1:-1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X)\n",
    "\n",
    "estimators, mean_res = eval_classifiers(X_train_, y, labels = labels) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 33.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bcc': 0.6697197640117993, 'mel': 0.5908634538152611, 'scc': 0.30521276595744634} 3.0684493036777345e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_data = pd.read_csv('../features/all/features_val_HSV_GLCM_shape_MC.csv')\n",
    "\n",
    "val_data_exc = val_data[val_data['label'].isin(list(category_mapping.keys()))]\n",
    "\n",
    "y =  val_data_exc['label'].astype('category').map(category_mapping)\n",
    "\n",
    "X = val_data_exc.iloc[:, 1:-1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y)\n",
    "\n",
    "# Standardization\n",
    "X_val_ = scaler.transform(X)\n",
    "\n",
    "print(\"null accuracy:\", 1/3 *100 )   \n",
    "\n",
    "acc_scores = {'bcc': 0.0, 'mel': 0.0, 'scc': 0.0}\n",
    "chk_score = 0.0\n",
    "\n",
    "for estimator in tqdm(estimators):\n",
    "    y_pred = estimator.predict(X_val_) \n",
    "    for i in range(3):\n",
    "        lab = list(category_mapping.keys())[i]\n",
    "        y_test_slice = y[y == i]\n",
    "        y_pred_slice = y_pred[y == i]\n",
    "        # print(i, len(y_test_slice), len(y_pred_slice), np.unique(y_test_slice), np.unique(y_pred_slice))\n",
    "        acc_scores[lab] += accuracy_score(y_test_slice, y_pred_slice)\n",
    "    \n",
    "    chk_score += cohen_kappa_score(y, y_pred, labels=labels)        \n",
    "\n",
    "for k, v in acc_scores.items():\n",
    "\n",
    "    acc_scores[k] = v / len(estimators)\n",
    "\n",
    "    chk_score /= len(estimators)\n",
    "\n",
    "print(acc_scores, chk_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy_score</th>\n",
       "      <th>test_cohen_kappa_score</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier(n_neighbors=1)</th>\n",
       "      <td>3.780224</td>\n",
       "      <td>0.063602</td>\n",
       "      <td>0.603918</td>\n",
       "      <td>0.293337</td>\n",
       "      <td>0.604114</td>\n",
       "      <td>0.603918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     fit_time  score_time  \\\n",
       "KNeighborsClassifier(n_neighbors=1)  3.780224    0.063602   \n",
       "\n",
       "                                     test_accuracy_score  \\\n",
       "KNeighborsClassifier(n_neighbors=1)             0.603918   \n",
       "\n",
       "                                     test_cohen_kappa_score  test_f1_score  \\\n",
       "KNeighborsClassifier(n_neighbors=1)                0.293337       0.604114   \n",
       "\n",
       "                                     test_recall_score  \n",
       "KNeighborsClassifier(n_neighbors=1)           0.603918  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nevus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
